{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#统计学习笔记（04）：KNN\" data-toc-modified-id=\"统计学习笔记（04）：KNN-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>统计学习笔记（04）：KNN</a></span><ul class=\"toc-item\"><li><span><a href=\"#k临近算法\" data-toc-modified-id=\"k临近算法-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>k临近算法</a></span></li><li><span><a href=\"#距离度量\" data-toc-modified-id=\"距离度量-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>距离度量</a></span></li><li><span><a href=\"#k的选择\" data-toc-modified-id=\"k的选择-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>k的选择</a></span></li><li><span><a href=\"#分类决策规则\" data-toc-modified-id=\"分类决策规则-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>分类决策规则</a></span></li><li><span><a href=\"#KNN的回归算法*\" data-toc-modified-id=\"KNN的回归算法*-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>KNN的回归算法*</a></span><ul class=\"toc-item\"><li><span><a href=\"#参考资料：\" data-toc-modified-id=\"参考资料：-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>参考资料：</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 统计学习笔记（04）：KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### k临近算法\n",
    "k临近算法（k-nearest neighbor,k-NN）是一种基本的分类与回归算法，k值的选择、距离度量及分类决策规则是k临近算法的三个基本要素。  \n",
    "输入：训练数据集 $T = {(x_1,y_1),(x_2,y_2).\\cdots,(x_N,y_N)}$，其中 $x_i \\in X \\subseteq R^n$ 为实例的特征向量， $y_i \\in Y = \\{c_1,c_2, \\cdots, c_k)$ 为实例的类别，$ i = 1,2, \\cdots, N$；  \n",
    "输出：实例 $x$ 所属的类 $y$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（1）根据给定的**距离度量**，在训练数据集 $T$ 中找出与 $x$ 最临近的 $k$ 个点，涵盖这 $k$ 个点的 $x$ 的邻域记作 $N_k (x)$；  \n",
    "（2）在 $N_k (x)$ 中根据**分类决策规则**（如多数表决）决定 $x$ 的类别 $y$：$$ y = arg\\,\\max_{c_j} \\sum_{x \\in N_k(x)} I(y_i = c_j), i = 1,2,, \\cdots N; j = 1,2,\\cdots, K $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中 $I$ 为指示函数（Indecator），即当 $y_i = c_j$ 时 $I$ 为 $1$，否则 $I$ 为 $0$。  \n",
    "k临近算法的特殊情况是 $k=1$ 的情形，称为最临近算法。对于输入的实例点（特征向量）$x$ ，最临近算法将训练数据集中与 $x$ 最临近点的类作为 $x$ 的类。   \n",
    "k临近算法**没有显式的学习过程**。当训练集、距离度量（如欧氏距离）、k值及分类决策规则（如多数表决）确定后，对于任何一个新的输入实例，它所属的类唯一地确定，这相当于根据上述要素将特征空间划分为一些子空间，确定子空间里的每个点所属的类。  \n",
    "特征空间中，对每个训练实例点 $x_l$ ，距离该点比其他点更近的所有点组成一个区域，叫做单元（cell），每个训练实例点拥有一个单元，所有训练实例点的单元构成对特征空间的一个划分，最近邻算法将实例 $x_l$ 的类 $y_l$ 作为其单元中所有点的类标记（class label），这样，每个单元的实例点的类别是确定的。  \n",
    "k临近算法使用的模型实际上对应于对特征空间的划分。模型由三个基本要素——**距离度量、k值的选择和分类决策规则**决定。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 距离度量\n",
    "特征空间中两个实例点的距离是两个实例点相似程度的反映。k临近模型的特征空间一般是 $n$ 维实数向量空间 $R^n$，使用的是欧氏距离。   \n",
    "设特征空间 $X$ 是 $n$ 维实数向量空间 $R^n$，$x_i,x_j \\in X, x_i = (x{_i}^{(1)},x{_i}^{(2)}, \\cdots ,x{_i}^{(n)} )^T$ ， $x_j = (x{_j}^{(1)},x{_j}^{(2)}, \\cdots ,x{_j}^{(n)} )^T$，$x_i,x_j$ 的 $L_p$ 距离定义为： $$L_p (x_i,x_j) = ( \\sum_{l=1}^{n} |x{_i}^{l} - x_{j}^{l} |^p)^{\\frac{1}{p}} $$\n",
    "这里 $ p \\geq 1 $，当 $p = 2$ 时，称为欧氏距离（Euclidean distenxe）； $p=1$ 时，称为曼哈顿距离（Manhattan distence）；当 $p= \\infty$ 时，它是各个坐标距离的最大值，  \n",
    "下图给出了二维空间中 $p$ 取不同值时，与原点的 $L_p$ 的距离为1 $(L_p = 1)$ 的点的图形。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### k的选择\n",
    "k值的选择会对k近邻法的结果产生重大影响——如果选择较小的k值，就相当于用较小的邻域中的训练实例进行预测，学习的近似误差（approximation error）会减少，只有与输入实例较近的（相似的）训练实例才会对预测结果起作用，但缺点是学习的估计误差（estimation error）会增大，预测结果会对近邻的实例点非常敏感。如果近邻的实例点恰巧是噪声，预测就会出错。即k值得减小就意味着整体模型变得更复杂，容易发生过拟合。  \n",
    "反之，如果选择较大的k值，相当于用较大的邻域中的训练实例进行预测，优点是可以减少学习的估计误差，但缺点是学习的近似误差会增加，因为这时与输入实例较远（不相似的）训练实例也会对预测起作用。k值得增大就意味着整体的模型变得简单。   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 分类决策规则\n",
    "k近邻法中的分类决策规则往往是多数表决，即由输入实例的k个邻近的训练实例中的多数类决定输入实例的类。  \n",
    "多数表决规则（majority voting rule）有如下解释：如果分类的损失函数为 $0-1$ 损失函数，分类函数为： $$ f: R^n \\rightarrow \\{c_1,c_2, \\cdots, c_K\\}$$  \n",
    "那么误分类的概率是 $$ P(Y \\ne f(X)) = 1 - P(Y = f(X))$$  \n",
    "对给定的实例 $x \\in X$，其最近邻的k个训练实例点构成的集合 $N_k (x)$ ，如果涵盖 $N_k (x)$ 的区域的类别是 $c_j$ ，那么误分类的概率是 $$ \\frac{1}{k}\\sum_{x_i \\in N_k (x)}I(y_i \\ne c_j) = 1 - \\frac{1}{k}\\sum_{x_i \\in N_k (x)}I(y_i = c_j)$$  \n",
    "要使误分类率最小即经验风险最小，就要使 $\\sum_{x_i \\in N_k (x)}I(y_i = c_j)$ 最大，所以多数表决规则等价于经验风险最小化。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNN的回归算法*   \n",
    "KNN不仅是分类方法，也可以用来做回归；基本思路是仍然按照KNN基本计算方法找到一个输入实例最临近的k个点，进而将这些最临近点对应值$y_n$的均值作为实例的回归结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 参考资料：  \n",
    "* [统计学习方法（第二版）](https://book.douban.com/subject/33437381/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
