{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#统计学习笔记（02）：感知机与支持向量机\" data-toc-modified-id=\"统计学习笔记（02）：感知机与支持向量机-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>统计学习笔记（02）：感知机与支持向量机</a></span><ul class=\"toc-item\"><li><span><a href=\"#感知机\" data-toc-modified-id=\"感知机-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>感知机</a></span><ul class=\"toc-item\"><li><span><a href=\"#感知机的定义\" data-toc-modified-id=\"感知机的定义-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>感知机的定义</a></span></li><li><span><a href=\"#关于向量和超平面的几个基础概念\" data-toc-modified-id=\"关于向量和超平面的几个基础概念-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>关于向量和超平面的几个基础概念</a></span></li><li><span><a href=\"#感知机的学习策略\" data-toc-modified-id=\"感知机的学习策略-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>感知机的学习策略</a></span></li><li><span><a href=\"#感知机的学习算法\" data-toc-modified-id=\"感知机的学习算法-1.1.4\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span>感知机的学习算法</a></span></li><li><span><a href=\"#感知机算法收敛性的证明：Novikoff定理\" data-toc-modified-id=\"感知机算法收敛性的证明：Novikoff定理-1.1.5\"><span class=\"toc-item-num\">1.1.5&nbsp;&nbsp;</span>感知机算法收敛性的证明：Novikoff定理</a></span></li><li><span><a href=\"#感知机的对偶算法\" data-toc-modified-id=\"感知机的对偶算法-1.1.6\"><span class=\"toc-item-num\">1.1.6&nbsp;&nbsp;</span>感知机的对偶算法</a></span></li><li><span><a href=\"#参考资料：\" data-toc-modified-id=\"参考资料：-1.1.7\"><span class=\"toc-item-num\">1.1.7&nbsp;&nbsp;</span>参考资料：</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 统计学习笔记（02）：感知机与支持向量机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 感知机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  感知机的定义\n",
    "**感知机定义**：假设输入空间（特征空间）是$X$ $\\subseteq$ $R^n$，输出空间是$Y$ = \\{-1,+1\\}，输入$x$ $\\in$ $X$表示实例的特征向量，对应于输入空间（特征空间）的点；$y$ $\\in$ $Y$ 表示实例的类别，有输入空间到输出空间的如下函数：  \n",
    "$$f(x) = sign(w\\cdot x + b) $$\n",
    "称为感知机。  \n",
    "\n",
    "感知机函数中，$w$和$b$为模型参数，其中$w$ $\\in$ $R^n$是权值（weight）或权值向量（weight vector），$b$ $\\in$ $R$ 称为偏置（bias）,$w\\cdot x$ 表示 $w$和$b$的内积，$sign$是符号函数，即：\n",
    "$$sign(y) =\\begin{cases} +1,  x\\geq0\\\\ -1,x < 0 \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感知机是一种分类模型，假设空间是定义在特征空间中所有的线性分类模型（linear classification model），或称线性分类器（linear classifier），即函数集合$\\{f \\mid f(x) = w\\cdot x +b \\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**感知机的几何理解**，以下图为例，方程 $w\\cdot x + b = 0$ 对应于二维空间中分离超平面 $S$（separating hyperplane），其中$w$是超平面的法向量，决定了超平面的方向，$b$是超平面的截距。 $S$ 将空间划分为两个部分，位于两部分的点（特征向量）被分为正、负两类。下图中圆点为正例，×为负例。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![perceptron](PICS\\perceptron01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  关于向量和超平面的几个基础概念\n",
    "\n",
    "**向量**：对于空间中的样本点 $x \\in R^n$ （n表示向量的维数）可以看做是一个向量，只要默认向量的的起点为原点 $0$ 就可以。  \n",
    "\n",
    "**超平面**：简单层面的理解，是从 $n$ 维空间到 $n-1$ 维空间的一个映射子空间，也即比所研究的空间低一个维度的子空间。超平面能够将一个空间分为两个部分。hyperplane中“hyper”是指对于3维以上的空间而言这种关系仍然存在。更严格的[定义](https://en.wikipedia.org/wiki/Hyperplane)是：  \n",
    "假设存在一组不全为0的标量 $a_1$，$a_2$，$a_3$，$a_4$ ... $a_n$ 而 $R^n$ 中一个向量集合$X = [x_1, x_2,x_3, x_4 ... x_n]^T$，$x_i \\in R^n$，满足：\n",
    "$$a_1x_1 + a_2x_2 + a_3x_3 + ... + a_nx_n = c $$\n",
    "$c$是一个常数，由满足上一等式的 $X$ 构成的 $R^n$ 向量子空间称为一个超平面（hyperplane）。这里假设其中一个$x_k \\in X$其权重$a_k \\neq 0$，则：\n",
    "$$x_k = \\frac {c - \\sum\\limits_{i=1}^{i\\neq k}a_ix_i}{a_k} $$\n",
    "可以发现，对于超平面 $X$ 而言，仅需要$n - 1$个向量即可以确定集合中剩下的一个 $x_k$ ，也就是说，至少存在一个 $x_k$，可以由其它元素得以确定，如果原始空间 $R^n$，其内的一个超平面其实是一个子空间，自由度最高为 $n-1$。  \n",
    "在感知机模型的例子中，对应地，为了将空间 $R^n$ 分为两个部分，需要一个超平面 $w\\cdot x + b = 0$ 其中 $x \\in R^n$。  \n",
    "**向量内积**：有时也称为向量的“点积”或“数量积”，定义为向量对应元素相乘后求和，即：$ A \\in R^n, B \\in R^n$ ， $$ A \\cdot B = \\sum a_i \\cdot b_i $$  \n",
    "内积的结果是一个标量，在二维和三维空间中，内积几何意义是 $A$ 和 $B$ 的模乘以二者的夹角的$\\cos \\theta$。即：$$A \\cdot B = \\Vert A \\Vert \\cdot \\Vert B \\Vert \\cdot \\cos \\theta$$  \n",
    "由于$\\cos \\theta \\in [-1,+1]$，以上式子也可以说明 $A \\cdot B \\le \\Vert A \\Vert \\cdot \\Vert B \\Vert$  \n",
    "\n",
    "**法向量**：normal vector，能决定一个平面方向的向量。在平面或三维中，平面与平面法向量的关系可以直接观察为“垂直”，更高维度中，平面与其法向量的关系可以表现为对平面内任意的一个向量 $x$，平面法向量 $w$ 与 $x$ 的关系是：$ w \\cdot x = 0$  \n",
    "关于在 $w \\cdot x + b = 0$ 中，$w$为这个超平面法向量的证明，以三维空间中为例：  \n",
    "取超平面内两个向量$x_1$和$x_2$，则有 $w \\cdot x_1 + b = 0$ 和 $w \\cdot x_2 + b = 0$，两式相建，就有了$w \\cdot (x_2 - x_1) = 0$，因为$x_1$和$x_2$均为超平面内的向量，则可知$x_2 - x_1$也是超平面内的向量，就证明了 $w$与任意超平面内向量的内积都是0。  \n",
    "\n",
    "**向量的模与范数** ： 向量的模可以理解为向量的长度，在平面和三维空间中可以非常直观地看出。向量的模一般记为 $|\\vec{a}|$，如果设 $\\vec{a} = (a_1,a_2,...a_n)$，则 $\\vec{a}$ 的模 $|\\vec{a}| = \\sqrt{a_1^2 + a_2^2 + ... + a_n^2}$  \n",
    "向量的范数是 $L_p = |x|_p = (\\sum_{i=1}^{i}|{x_i}|^p)^\\frac{1}{p}$，这样向量的模也可以被称为$L_2$范数。  \n",
    "\n",
    "**点到一个给定平面距离公式**：假设平面 $w \\cdot x + b = 0$ 外有点 $x_0$，其到这个平面的距离为：  \n",
    "$$\\frac{|w \\cdot x_0 +b|}{\\Vert w\\Vert} $$\n",
    "  \n",
    "简单证明，可以假设平面为一个直线或三维空间中的面，通过联立方程求解。另外可以通过利用法向量 $w$ 的方式证明：  \n",
    "设超平面 $w \\cdot x + b = 0 $ 外有一点 $x_0(x_1,x_2,...x_i)$，其在平面上的[投影](https://en.wikipedia.org/wiki/Projection_(linear_algebra))为 $x_1(x_1^`,x_2^`,...x_i^`)$，则$|\\vec{x_0 x_1}|$即 $x_0$到平面的距离，设其为$d$。根据法向量的定义可以知道，$\\vec{x_1 x_0}$与 $w(w_0,w_1,...w_i)$是平行的，则有式(1)： $$w \\cdot\\vec{x_1 x_0}=  |w| \\cdot|\\vec{x_1 x_0}|\\cdot cos0 = |w \\cdot \\vec{x_1 x_0}| = |w|\\cdot d $$  \n",
    "计算 $w$ 和 $\\vec{x_0 x_1}$ 的内积又有：\n",
    "$$ w \\cdot \\vec{x_0 x_1} = w_1(x_1 - x_1^`) + w_1(x_2 - x_2^`) + \\cdots + w_1(x_i - x_i^`)  = (w_1 \\cdot x_1 + w_2 \\cdot x_2 + \\cdots +w_i \\cdot x_i) - (w_1 \\cdot x_1^` + w_2 \\cdot x_2^` + \\cdots +w_i \\cdot x_i^`) $$\n",
    "  \n",
    "$$ \\because x_1 是平面 {w \\cdot x + b = 0}中的向量，由法向量的性质可以推出 w \\cdot x_1 + b = 0 $$ \n",
    "\n",
    "即$ w \\cdot x_1 = -b $\n",
    "\n",
    "$$ \\therefore  w \\cdot \\vec{x_1 x_0} = (w_1 \\cdot x_1 + w_2 \\cdot x_2 + \\cdots +w_i \\cdot x_i) - (-b) = (w_1 \\cdot x_1 + w_2 \\cdot x_2 + \\cdots +w_i \\cdot x_i) + b $$  \n",
    "\n",
    "则有$w \\cdot \\vec{x_1 x_0} = w \\cdot x_0 + b $  \n",
    "结合式(1)，可以得到 $|w \\cdot x_0 + b| = |w|\\cdot d $，则$ d = \\frac{|w \\cdot x_0 + b|}{|w|}$，综合对模和$L_2$范数的定义，就得到了：$$\\frac{|w \\cdot x_0 +b|}{||w||} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  感知机的学习策略\n",
    "定义**线性可分数据集**：对于一个数据集 $T = {(x_1,y_1),(x_2,y_2).\\cdots,(x_N,y_N)}$，其中 $ x_i \\in X = R^n $， $y_i \\in Y = \\{+1,-1\\}$，$ i = 1,2,\\cdots , N$，如果存在某个超平面S\n",
    "$$ w \\cdot x +b =0 $$\n",
    "能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，即对所有 $y_i = +1$的实例，有 $w \\cdot x +b>0$，对所有 $y_i = -1$的实例，有 $w \\cdot x +b<0$，则称数据集 $T$ 是线性可分数据集（linearly separable data set）；否则，称数据集 $T$ 线性不可分。  \n",
    "数据集线性可分的充分必要条件是正实例点所构成的凸壳与负实例点集所构成的凸壳互不相交。其中凸壳的定义是：  \n",
    "设集合 $ S \\subseteq R^n$是由 $R^n$ 中的k个点所组成的集合，即 $S=\\{x_1,x_2,\\cdots,x_k \\}$， $S$ 的凸壳 $conv(S)$为 $$ conv(S) = \\lbrace x=\\sum_{i=0}^k \\lambda_i x_i |\\sum_{i=0}^k \\lambda_i = 1, \\lambda_i \\geq 0, i = 1,2,\\cdots, k \\rbrace$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**策略**是在确定了模型的假设空间后，选择什么样的准则学习或选择最优模型，因此需要引入损失函数或风险函数来度量一个模型的好坏。对感知机而言，损失函数一个自然的选择就是误分类点的个数，但这个“损失函数”不是参数 $w,b$ 的连续可导函数，不易优化。损失函数的另一个选择时误分类点到超平面 $S$ 的总距离，这是感知机所使用的策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "平面外一点 $x_0$ 到平面 $w \\cdot x +b = 0$ 的距离是：$$\\frac{|w \\cdot x_0 +b|}{||w||} $$  \n",
    "对于误分类点 $(x_i,y_i)$ 而言，$ | w \\cdot x_i + b | = -y_i (w \\cdot x_i + b)$，即对于误分类点 $(x_i,y_i)$，如果 $y_i=+1$ ， $ w\\cdot x_i + b < 0 $， 如果 $y_i=-1$ ，$ w\\cdot x_i + b > 0 $。因此，误分类点 $(x_i,y_i)$ 到超平面的距离是 $ -\\frac {1}{||w||} \\cdot y_i (w \\cdot x_i + b)$，所有误分类点到超平面距离的总和是：$$ -\\frac {1}{||w||} \\cdot \\sum_{x\\in M} y_i (w \\cdot x_i + b) $$  \n",
    "不考虑 $ \\frac {1}{||w||}$ 就得到了感知机的损失函数：$$ L(w,b) = \\sum_{x_i \\in M} -y_i(w\\cdot x_i +b)$$ \n",
    "以上 $M$是误分类点的集合。以上损失函数中，$y_i(w\\cdot x_i +b)$ 被称为**函数间隔**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感知机的损失函数是非负的，当没有误分类点时，$L(w,b) = 0$，误分类点越少，与超平面距离越近，损失函数的值越小（越接近0）。一个特定样本点在被误分类时，损失函数是关于 $w$ 和 $b$ 的线性函数，正确分类时则是0，可以通过导数求梯度，从而减小损失函数的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  感知机的学习算法\n",
    "感知机学习策略是求解下式的最小值： $$ L(w,b) = \\sum_{x_i \\in M} -y_i(w\\cdot x_i +b)$$ \n",
    "其中 $M$ 是误分类点的集合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感知机的学习算法是误分类驱动的，采用的是随机梯度下降法（stochastic gradient descent, SGD）。首先任意选取一个超平面 $w_0,b_0$， 然后用梯度下降方法不断使损失函数 $L(w,b)$ 值极小化。极小化过程不是一次使 $M$ 中所有的误分类点的梯度下降，而是一次随机选取一个误分类点使其梯度下降。  \n",
    "对于感知机模型，将线性可分的数据集分成正确地两部分的超平面可能有多个，因此用误分类点进行随机梯度下降相较批量进行梯度下降计算更合适，同时由于 $L(w,b)$ 是线性函数，这也是损失函数用“极小化”而不是“最小化”的原因，对于不同的初始选择 $w,b$，最终可能得到不同的分离超平面。下一部分也将证明，随机梯度下降的迭代次数是有限的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设误分类点集合 $M$ 是固定的，那么损失函数 $L(w,b)$ 的梯度由：$$ \\nabla_w L(w,b) = - \\sum_{x_i \\in M} y_i x_i$$ $$ \\nabla_b L(w,b) = - \\sum_{x_i \\in M} y_i $$  \n",
    "给出，其中 $\\nabla$ 表示梯度，即函数的偏导数，其方向是向“上”的，因此对于梯度下降，应当朝向的向量是负梯度方向。  \n",
    "感知机的算法中，首先选取一个误分类点 $(x_i, y_i)$ 对 $w,b$ 进行更新：$$ w \\gets w + \\eta y_i x_i \\quad  (1)$$  $$ b \\gets \\eta y_i  \\quad  (2)  $$  \n",
    "$\\eta(0 < \\eta \\leq 1)$ 是步长，也称“学习率”（learning rate）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**感知机算法的原始形式**  \n",
    "输入：训练数据集 $T = \\{(x_1,y_1),(x_2, y_2), \\cdots ,(x_N,y_N)\\}$ ,其中 $x_i \\in X = R^n，y_i \\in Y = \\{-1,+1\\}，i = 1,2,\\cdots, N$ ；学习率 $\\eta(0 < \\eta \\leq 1)$ ；  \n",
    "输出：$w,b$ ；感知机模型 $f(x) = sign(w \\cdot x +b) $。  \n",
    "\n",
    "（1）选取初值 $w_0, b_0$  \n",
    "（2）在训练集中选取数据 $(x_i,y_i)$  \n",
    "（3）如果 $ y_i (w \\cdot x_i +b) \\leq 0 $ ： $$ w \\gets w + \\eta y_i x_i$$  $$ b \\gets \\eta y_i$$  \n",
    "（4）转至（2），直至训练集中没有误分类点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  感知机算法收敛性的证明：Novikoff定理\n",
    "**Novikoff定理**证明了对于线性可分数据集感知机算法的原始形式收敛，即经过有限次迭代可以得到一个将训练数据集完全正确划分的分离超平面及感知机模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在推导中，为便于叙述，将偏置 $b$ 并入权重向量 $w$，记作 $ \\hat{w} = (w^T, b)^T$，同样也将输入向量加以扩充，加进常数 $1$，，记作 $ \\hat{x} = (x^T,1)^T$，这样，$\\hat{x} \\in R^{n+1}$，$\\hat{w} \\in R^{n+1}$，显然有 $\\hat{w} \\cdot \\hat{x} = w \\cdot x +b$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Novikoff定理**：  \n",
    "设训练数据集 $T = \\{(x_1,y_1),(x_2, y_2), \\cdots ,(x_N,y_N)\\}$ 是线性可分的，其中 $x_i \\in X = R^n，y_i \\in Y = \\{-1,+1\\}，i = 1,2,\\cdots, N$，则：  \n",
    "  \n",
    "（1）存在满足条件 $ \\|\\hat{w}_{opt}\\|=1$ 的超平面 $\\hat{w}_{opt} \\cdot \\hat{x} = w_{opt} \\cdot x +b_{opt}=0$ 将训练集完全正确分开；且存在 $\\gamma > 0 $，对于所有 $i = 1,2,\\cdots, N$：$$y_i (\\hat{w}_{opt} \\cdot \\hat{x}) = y_i (w_{opt} \\cdot x +b_{opt}) \\geq \\gamma$$  \n",
    "  \n",
    "（2）令 $ R = \\max\\limits_{1 \\geq i \\geq N} {\\|\\hat{x_i} \\|}$，则感知机在训练数据集上误分类次数 $k$ 满足不等式： $$k \\leq {(\\frac{R}{\\gamma})}^2$$  \n",
    "\n",
    "证明：  \n",
    "（1）数据集是线性可分的，根据感知机定义，存在超平面将数据集完全正确分开，取一个符合的超平面为 $\\hat{w}_{opt} \\cdot \\hat{x} = w_opt \\cdot x + b_{opt} = 0$ , 由于对**有限个**数据集中的样本（实例），即 $ i = 1,2,\\cdots,N$，均有$$y_i(\\hat{w}_{opt} \\cdot \\hat{x}) = y_i(w_{opt} \\cdot x + b_{opt})\\gt 0$$  \n",
    "所以存在$$\\gamma = \\min_{i}\\{y_i(\\hat{w}_{opt} \\cdot \\hat{x})\\}$$  \n",
    "使$$y_i(\\hat{w}_{opt} \\cdot \\hat{x}) = y_i(w_{opt} \\cdot x + b_{opt}) \\ge \\gamma$$  \n",
    "\n",
    "（2）感知机算法从 $\\hat{w}_{0} = 0$开始，若实例被误分类，则更新权重，如果令$\\hat{w_{k-1}}$是第k个误分类实例之前的扩充权重向量，即$$\\hat{w}_{k-1} = (w_{k-1}^T, b_{k-1})^T$$  \n",
    "则第k个误分类实例的条件是：$$y_i(\\hat{w}_{k-1} \\cdot \\hat{x}_i) = y_i(w_{k-1} \\cdot x_i + b_{k-1}) \\le 0$$  \n",
    "若$(x_i,y_i)$是误分类点，则$w$和$b$的更新是：$$w_k \\gets w_{k-1} + \\eta y_i x_i$$ $$b_k \\gets b_{k-1} + \\eta y_i$$  \n",
    "即：$$\\hat{w}_k = \\hat{w}_{k-1} + \\eta y_i \\hat{x}_i$$  \n",
    "\n",
    "结合第（1）部分的结论，$y_i(\\hat{w}_{opt}\\cdot\\hat{x})$ 存在一个恒大于等于的 $\\gamma$，由以上可以推导：\n",
    "$$\\hat{w}_k \\cdot \\hat{w}_{opt} = \\hat{w}_{k-1}\\cdot \\hat{w}_{opt} + \\eta y_i \\hat{w}_{opt} \\cdot\\hat{x}_i \\ge \\hat{w}_{k-1}\\cdot \\hat{w}_{opt} + \\eta \\gamma$$  \n",
    "继续向前推导上式，可以得到：\n",
    "$$ \\hat{w}_k \\cdot \\hat{w}_{opt} \\ge \\hat{w}_{k-1}\\cdot \\hat{w}_{opt} + \\eta \\gamma \\ge\\hat{w}_{k-2}\\cdot \\hat{w}_{opt} + 2\\eta \\gamma \\ge \\cdots \\ge k\\eta\\gamma$$  \n",
    "（注意一开始的约定是“感知机算法从$\\hat{w}_{0} = 0$开始”）  \n",
    "\n",
    "接下来，对$\\hat{w}_k = \\hat{w}_{k-1} + \\eta y_i \\hat{x}_i$扩展，计算$\\Vert \\hat{w}_k \\Vert ^2$：\n",
    "$$\\Vert \\hat{w}_k \\Vert ^2 = \\Vert \\hat{w}_{k-1}\\Vert^2 + \\eta^2 \\Vert \\hat{x}_{i}\\Vert ^2 + 2\\eta y_i \\hat{w}_{k-1} \\cdot \\hat{x}_i$$  \n",
    "因为$(x_i,y_i)$对目前超平面是一个误分类点，因此$y_i \\hat{w}_{k-1} \\cdot \\hat{x}_i \\le 0$，所以$$\\Vert \\hat{w}_k \\Vert ^2 \\le \\Vert \\hat{w}_{k-1}\\Vert^2 + \\eta^2 \\Vert \\hat{x}_{i}\\Vert ^2 $$  \n",
    "又在一开始设$ R = \\max\\limits_{1 \\ge i \\ge N} {\\|\\hat{x_i} \\|}$，所以有：\n",
    "$$\\Vert \\hat{w}_k \\Vert ^2 \\le \\Vert \\hat{w}_{k-1}\\Vert^2 + \\eta^2 R^2 \\le \\Vert\\hat{w}_{k-2}\\Vert^2 + 2\\eta^2 R^2 \\le \\cdots \\le \\Vert\\hat{w}_{0}\\Vert^2 + k\\eta^2 R^2 $$ \n",
    "即$$\\Vert \\hat{w}_k \\Vert ^2 \\le k\\eta^2 R^2 $$  \n",
    "\n",
    "以上过程证明了：  \n",
    "$$ \\hat{w}_k \\cdot \\hat{w}_{opt} \\ge k\\eta\\gamma$$  \n",
    "$$ \\Vert \\hat{w}_k \\Vert \\le \\sqrt{k} \\eta R$$  \n",
    "\n",
    "设定 $ \\Vert \\hat{w}_{opt} \\Vert = 1$ （对于一个形如 $w \\cdot x + b = 0$ 的式子，总能令 $\\Vert w \\Vert = 1$），根据上式，则有：\n",
    "$$ k \\eta \\gamma \\le \\hat{w}_k \\cdot \\hat{w}_{opt} \\le \\Vert \\hat{w}_k \\Vert \\Vert \\hat{w}_{opt} \\Vert \\le \\sqrt{k} \\eta R$$  \n",
    "即$$ k \\eta \\gamma \\le \\sqrt{k} \\eta R$$ \n",
    "也就是：$$k \\leq {(\\frac{R}{\\gamma})}^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  感知机的对偶算法\n",
    "\n",
    "对偶形式的基本思路是，将 $w$ 和 $b$ 表示维实例 $x_i$ 和标记 $y_i$ 的线性组合的形式，通过求解其系数得到 $w$ 和 $b$ 。这里先使初始值 $w_0$ 和 $b_0$ 均为0，对误分类点 $(x_i,y_i)$ 通过 $$w \\gets w + \\eta y_i x_i$$  $$b \\gets b + \\eta y_i$$  \n",
    "逐步修改 $w$ 和 $b$ ，设修改 $n$ 次，则 $w$ ， $b$ 关于 $(x_i,y_i)$ 的增量分别是 $\\alpha_i y_i x_i$ 和 $\\alpha_i y_i$，这里 $\\alpha_i  = n_i \\eta$，在学习过程中，最后学习到的 $w$ ， $b$ 是 $$w = \\sum_{i =1}^{N} \\alpha_i y_i x_i$$  $$b = \\sum_{i =1}^{N} \\alpha_i y_i$$\n",
    "这里，$ \\alpha_i \\ge 0$，$i = 1,2,3,\\cdots, N$，当 $\\eta=1$ 时，$\\alpha_i$表示第 $i$ 个实例点由于误分类而更新的次数，实例点更新次数越多，意味着它距离分离超平面越近，也就越男正确分类，这样的实例点对学习结果的影响也就越大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**感知机算法的对偶形式**  \n",
    "输入：线性可分数据集$T = \\{(x_1,y_1),(x_2,y_3),\\cdots,(x_N,y_N)\\}$，其中，$x_i \\in R^n$，$y_i \\in \\{+1,-1\\}$，$i = 1,2,\\cdots,N$；学习率 $\\eta$ $(0 \\lt \\eta \\le 1)$ ；  \n",
    "输出：$a,b$；感知机模型 $f(x) = sign\\lgroup \\sum\\limits_{j=1}^N a_j y_j x_j \\cdot x + b \\rgroup$  \n",
    "其中 $a = (a_1,a_2,\\cdots,a_N)^T$  \n",
    "（1） $a \\gets 0，b \\gets 0$  \n",
    "（2） 在训练集中选取数据 $(x_i,y_i)$  \n",
    "（3） 如果 $y_i \\lgroup \\sum \\limits_{j=1}^N a_j y_j x_j \\cdot x_i +b \\rgroup \\le 0$  \n",
    "（4） 转至（2）直到没有误分类的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对偶形式中训练实例仅以内积的形式出现，为了方便，可以预先将训练集中的实例间的内积计算出来并以矩阵的形式储存，这个矩阵就是Gram矩阵（Gram matrix）:\n",
    "$$G = [x_i \\cdot x_j]_{N \\times N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 参考资料：  \n",
    "* [统计学习方法（第二版）](https://book.douban.com/subject/33437381/)\n",
    "* [超平面（hyperplane）的定义](https://blog.csdn.net/lanchunhui/article/details/53074100)\n",
    "* [线性代数笔记3——向量2（点积）](https://www.cnblogs.com/bigmonkey/p/8079386.html)\n",
    "* [点到平面的距离公式](https://www.cnblogs.com/graphics/archive/2010/07/10/1774809.html)\n",
    "* [对梯度概念的直观理解](https://www.v2ex.com/t/397822)\n",
    "* [感知机的对偶形式及Gram矩阵的作用理解](https://blog.csdn.net/qq_26826585/article/details/87967520)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
